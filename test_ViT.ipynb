{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "#from torchtext import data, datasets\n",
    "#from torchtext.vocab import Vocab\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from datagenerator import *\n",
    "from util_data import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Avaliable : False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data from WeatherBench\n",
    "DATADIR = '/Users/noeliaotero/Documents/CAS_ML/WeatherBench-master/data/WeatherBench/5.625deg/'\n",
    "# Load the entire dataset\n",
    "z500 = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords').z\n",
    "t850 = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').t.drop('level')\n",
    "ds = xr.merge([z500, t850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice('2015', '2016'))  \n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})\n",
    "lead_time =1\n",
    "bs = 32\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2015')), dic, lead_time, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2016', '2016')), dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 32, 64)\n",
      "torch.Size([32, 2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "X,y=dg_train[0]\n",
    "print(X.shape)\n",
    "Xt = torch.as_tensor(X)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and then embed them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_size :  Size of the image (my case: it is not a square).\n",
    "    patch_size : Size of the patch (my case: it is not a square).\n",
    "    in_chans : int Number of input channels.\n",
    "    embed_dim : int The emmbedding dimension. This determines how big an embeding of our patch is going to be.\n",
    "                Embedding stays constant across entire network ; flattened length for each token (or patch)\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches\n",
    "        and their embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        super().__init__()\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
    "        self.n_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        print(self.n_patches)\n",
    "        # here, we take the kernel size and the stride size as the patch_size, this way sliding \n",
    "        # the kernel along the input tensor, we'll never slide it in an overlappig way and the \n",
    "        # kernel will fall into these patches that we're trying to divide our image into.\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "              \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" Run forward pass\n",
    "        Parameters\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "            Shape(n_samples, in_chans, img_size, img_size)\n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor (n_smaples, n_patches, emb_dim) \"\"\"\n",
    "        # or ..Output shape: (batch size, no. of patches, no. of channels)\n",
    "        x = torch.as_tensor(X)\n",
    "        x = self.proj(x) # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2)  #  we take the last two dimmesions that represent the grid of patches and \n",
    "                          #  flatten them into single dim (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (n_samples, n_patches, embed_dim)\n",
    "\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        The input and out dimension of per token features.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors.\n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor.\n",
    "    Attributes\n",
    "    ----------\n",
    "    scale : float\n",
    "        Normalizing consant for the dot product.\n",
    "    qkv : nn.Linear\n",
    "        Linear projection for the query, key and value.\n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention\n",
    "        heads and maps it into a new space.\n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "    \n",
    "\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)  # (n_samples, n_patches + 1, 3 * dim)\n",
    "        qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "        ) \n",
    "        # (n_smaples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(\n",
    "                2, 0, 3, 1, 4\n",
    "        )  # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "       \n",
    "        q, k , v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "      \n",
    "        dp = (\n",
    "           q @ k_t\n",
    "        ) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(\n",
    "                1, 2\n",
    "        )  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features.\n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer.\n",
    "    out_features : int\n",
    "        Number of output features.\n",
    "    p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    fc : nn.Linear\n",
    "        The First linear layer.\n",
    "    act : nn.GELU\n",
    "        GELU activation function.\n",
    "    fc2 : nn.Linear\n",
    "        The second linear layer.\n",
    "    drop : nn.Dropout\n",
    "        Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches +1, out_features)`\n",
    "        \"\"\"\n",
    "        x = self.fc1(\n",
    "                x\n",
    "        ) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embeddinig dimension.\n",
    "    n_heads : int\n",
    "        Number of attention heads.\n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension size of the `MLP` module with respect\n",
    "        to `dim`.\n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value projections.\n",
    "    p, attn_p : float\n",
    "        Dropout probability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm1, norm2 : LayerNorm\n",
    "        Layer normalization.\n",
    "    attn : Attention\n",
    "        Attention module.\n",
    "    mlp : MLP\n",
    "        MLP module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        # dim = 64\n",
    "        super().__init__()\n",
    "       \n",
    "        self.dim = dim\n",
    "        self.n_head = n_heads\n",
    "        self.norm1 = nn.LayerNorm(self.dim, eps=1e-6)    #This is the first layer norm\n",
    "        # This is the MSA \n",
    "        self.attn = Attention(\n",
    "                self.dim,\n",
    "                n_heads=self.n_head,\n",
    "                qkv_bias=qkv_bias,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=p\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)   # second layer norm\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        # This is where the MLP layer comes in \n",
    "        self.mlp = MLP(\n",
    "                in_features=dim,\n",
    "                hidden_features=hidden_features,\n",
    "                out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape `(n_samples, n_patches + 1, dim)`.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 64])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PatchEmbedding((32,64), (16,16), 2, 64)\n",
    "p(Xt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTencoder(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, depth):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.n_heads = n_heads\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = nn.LayerNorm(self.embed_dim, eps=1e-6)\n",
    "        for _ in range(self.depth):\n",
    "            layer = Block(self.embed_dim, self.n_heads)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states = layer_block(hidden_states)\n",
    "\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=2,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge everything \n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=(32,64),\n",
    "            patch_size=(1,2), #(4,8),\n",
    "            in_chans= 2   ,   #2,    \n",
    "            embed_dim= 64,       #64,\n",
    "            depth=12,           #This is 12 for TransUNet\n",
    "            n_heads=8,\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.in_chans = in_chans\n",
    "        self.depth = depth\n",
    "        self.n_heads = n_heads\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "                img_size=self.img_size,\n",
    "                patch_size=self.patch_size,\n",
    "                in_chans=self.in_chans,\n",
    "                embed_dim=self.embed_dim,\n",
    "        )\n",
    "       \n",
    "        self.encoder = ViTencoder(self.embed_dim, self.n_heads, self.depth)\n",
    "        \n",
    "        self.down_factor = 0 # 4, 2\n",
    "        self.conv_more = Conv2dReLU(self.embed_dim, 128, kernel_size=1, use_batchnorm=True)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # embedding_output, features = self.embeddings(input_ids)\n",
    "        # encoded, attn_weights = self.encoder(embedding_output)  # (B, n_patch, hidden)\n",
    "        # return encoded, attn_weights, features\n",
    "        embedding_output = self.patch_embed(input_ids)\n",
    "        encoded = self.encoder(embedding_output)  # (B, n_patch, hidden)\n",
    "        print(encoded.shape)\n",
    "        B, n_patch, hidden = encoded.size()\n",
    "        l, h = (self.img_size[0] // 2**self.down_factor // self.patch_size[0]), (self.img_size[1] // 2**self.down_factor // self.patch_size[1])\n",
    "    \n",
    "        x = encoded.permute(0, 2, 1)\n",
    "        print(B)\n",
    "        print(hidden)\n",
    "        print(l)\n",
    "        print(h)\n",
    "        x = x.contiguous().view(B, hidden, l, h)\n",
    "\n",
    "        x = self.conv_more(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "p = PatchEmbedding((32,64), (2,4), 2, 128)\n",
    "img_size = (32,64)\n",
    "patch_size = (2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "encoder = ViT(img_size=(32,64), patch_size=(4,8), in_chans=2, embed_dim=64, depth = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32 // 2**0// 4), (64// 2**0 // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 64])\n",
      "32\n",
      "64\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x = encoder(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 4, 4])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = x.contiguous().view(32, 64, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 16, 16])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 8, 8])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2dReLU(64, 2, kernel_size=1, use_batchnorm=True)(yy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 32, 64])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CASM6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89acc57343a2b9c233d3a03eb936ce8fe371fffead8157434df0be7232f9cdb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
