{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.conda/envs/pyTT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-23 15:29:20.852551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 15:29:23.780061: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software.el7/software/cuDNN/8.2.1.32-CUDA-11.3.1/lib:/software.el7/software/CUDA/11.3.1/nvvm/lib64:/software.el7/software/CUDA/11.3.1/extras/CUPTI/lib64:/software.el7/software/CUDA/11.3.1/lib:/software.el7/software/Python/3.9.5-GCCcore-10.3.0/lib:/software.el7/software/OpenSSL/1.1/lib:/software.el7/software/libffi/3.3-GCCcore-10.3.0/lib64:/software.el7/software/GMP/6.2.1-GCCcore-10.3.0/lib:/software.el7/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/software.el7/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/software.el7/software/libreadline/8.1-GCCcore-10.3.0/lib:/software.el7/software/ncurses/6.2-GCCcore-10.3.0/lib:/software.el7/software/zlib/1.2.11-GCCcore-10.3.0/lib:/software.el7/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/software.el7/software/HDF5/1.12.1-gompi-2021a/lib:/software.el7/software/Szip/2.1.1-GCCcore-10.3.0/lib:/software.el7/software/ScaLAPACK/2.1.0-gompi-2021a-fb/lib:/software.el7/software/FFTW/3.3.9-gompi-2021a/lib:/software.el7/software/FlexiBLAS/3.0.4-GCC-10.3.0/lib:/software.el7/software/OpenBLAS/0.3.15-GCC-10.3.0/lib:/software.el7/software/OpenMPI/4.1.1-GCC-10.3.0/lib:/software.el7/software/libfabric/1.12.1-GCCcore-10.3.0/lib:/software.el7/software/UCX/1.10.0-GCCcore-10.3.0/lib:/software.el7/software/hwloc/2.4.1-GCCcore-10.3.0/lib:/software.el7/software/libpciaccess/0.16-GCCcore-10.3.0/lib:/software.el7/software/libxml2/2.9.10-GCCcore-10.3.0/lib:/software.el7/software/XZ/5.2.5-GCCcore-10.3.0/lib:/software.el7/software/numactl/2.0.14-GCCcore-10.3.0/lib:/software.el7/software/binutils/2.36.1-GCCcore-10.3.0/lib:/software.el7/software/GCCcore/10.3.0/lib64\n",
      "2023-01-23 15:29:23.780346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software.el7/software/cuDNN/8.2.1.32-CUDA-11.3.1/lib:/software.el7/software/CUDA/11.3.1/nvvm/lib64:/software.el7/software/CUDA/11.3.1/extras/CUPTI/lib64:/software.el7/software/CUDA/11.3.1/lib:/software.el7/software/Python/3.9.5-GCCcore-10.3.0/lib:/software.el7/software/OpenSSL/1.1/lib:/software.el7/software/libffi/3.3-GCCcore-10.3.0/lib64:/software.el7/software/GMP/6.2.1-GCCcore-10.3.0/lib:/software.el7/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/software.el7/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/software.el7/software/libreadline/8.1-GCCcore-10.3.0/lib:/software.el7/software/ncurses/6.2-GCCcore-10.3.0/lib:/software.el7/software/zlib/1.2.11-GCCcore-10.3.0/lib:/software.el7/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/software.el7/software/HDF5/1.12.1-gompi-2021a/lib:/software.el7/software/Szip/2.1.1-GCCcore-10.3.0/lib:/software.el7/software/ScaLAPACK/2.1.0-gompi-2021a-fb/lib:/software.el7/software/FFTW/3.3.9-gompi-2021a/lib:/software.el7/software/FlexiBLAS/3.0.4-GCC-10.3.0/lib:/software.el7/software/OpenBLAS/0.3.15-GCC-10.3.0/lib:/software.el7/software/OpenMPI/4.1.1-GCC-10.3.0/lib:/software.el7/software/libfabric/1.12.1-GCCcore-10.3.0/lib:/software.el7/software/UCX/1.10.0-GCCcore-10.3.0/lib:/software.el7/software/hwloc/2.4.1-GCCcore-10.3.0/lib:/software.el7/software/libpciaccess/0.16-GCCcore-10.3.0/lib:/software.el7/software/libxml2/2.9.10-GCCcore-10.3.0/lib:/software.el7/software/XZ/5.2.5-GCCcore-10.3.0/lib:/software.el7/software/numactl/2.0.14-GCCcore-10.3.0/lib:/software.el7/software/binutils/2.36.1-GCCcore-10.3.0/lib:/software.el7/software/GCCcore/10.3.0/lib64\n",
      "2023-01-23 15:29:23.780361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "#from torchtext import data, datasets\n",
    "#from torchtext.vocab import Vocab\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from datagenerator import *\n",
    "from util_data import * \n",
    "from metrics import *\n",
    "from helpers import *\n",
    "from drop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Avaliable : True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data from WeatherBench\n",
    "DATADIR = '/storage/homefs/no21h426/WeatherBench-master/data/WeatherBench/5.625deg/'\n",
    "# Load the entire dataset\n",
    "z500 = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords').z\n",
    "t850 = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').t.drop('level')\n",
    "ds = xr.merge([z500, t850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice('2015', '2016'))  \n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})\n",
    "lead_time =1\n",
    "bs = 32\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2015')), dic, lead_time, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2016', '2016')), dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 32, 64)\n",
      "torch.Size([32, 2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "X,y=dg_train[0]\n",
    "print(X.shape)\n",
    "Xt = torch.as_tensor(X)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            hidden_features=None,\n",
    "            out_features=None,\n",
    "            act_layer=nn.GELU,\n",
    "            bias=True,\n",
    "            drop=0.,\n",
    "            use_conv=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n",
    "\n",
    "        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, drop=drop)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            embed_dim=768,\n",
    "            norm_layer=None,\n",
    "            flatten=True,\n",
    "            bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "       # assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
    "       # assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")\n",
    "        # need to add\n",
    "        #x = torch.as_tensor(x)\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchE = PatchEmbed( [32,64], [16,16], 2, 64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test = torch.randn(2, 2, 32, 64)\n",
    "pt = patchE(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size_h, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size_w, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "# Ref.:https://github.com/rwightman/pytorch-image-models/\n",
    "\n",
    "\n",
    "def _trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    # Values are generated by using a truncated uniform distribution and\n",
    "    # then using the inverse CDF for the normal distribution.\n",
    "    # Get upper and lower cdf values\n",
    "    l = norm_cdf((a - mean) / std)\n",
    "    u = norm_cdf((b - mean) / std)\n",
    "\n",
    "    # Uniformly fill tensor with values from [l, u], then translate to\n",
    "    # [2l-1, 2u-1].\n",
    "    tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "    # Use inverse cdf transform for normal distribution to get truncated\n",
    "    # standard normal\n",
    "    tensor.erfinv_()\n",
    "\n",
    "    # Transform to proper mean, std\n",
    "    tensor.mul_(std * math.sqrt(2.))\n",
    "    tensor.add_(mean)\n",
    "\n",
    "    # Clamp to ensure it's in the proper range\n",
    "    tensor.clamp_(min=a, max=b)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n",
    "    applied while sampling the normal with mean/std applied, therefore a, b args\n",
    "    should be adjusted to match the range of mean, std args.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return _trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from climate_learn\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        in_vars, \n",
    "        out_vars,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "        learn_pos_emb=False,\n",
    "        upsampling=1,\n",
    "        embed_dim=128,\n",
    "        depth=12,\n",
    "        decoder_depth=8,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        weight_init = '',\n",
    "    ):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.upsampling = upsampling\n",
    "\n",
    "        self.img_out_size = [img_size[0] * upsampling, img_size[1] * upsampling]\n",
    "        self.n_channels = in_vars\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_vars = in_vars\n",
    "        self.out_vars = out_vars\n",
    "\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT encoder\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size, patch_size, in_vars, embed_dim\n",
    "        )\n",
    "        self.num_patches = self.patch_embed.num_patches  # 128\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.num_patches, embed_dim), requires_grad=learn_pos_emb\n",
    "        )  # fixed sin-cos embedding\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "           \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                 Block(\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    drop=drop_rate,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # Up to here, it's like I did before\n",
    "        # Next part introduces further changes \n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT prediction head\n",
    "        self.head = nn.ModuleList()\n",
    "        for i in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(\n",
    "            nn.Linear(embed_dim, self.out_vars * patch_size**2 * upsampling**2)\n",
    "        )\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "        # --------------------------------------------------------------------------\n",
    "        if weight_init != 'skip':\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.img_size[0] / self.patch_size),\n",
    "            int(self.img_size[1] / self.patch_size),\n",
    "            cls_token=False,\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # w = self.patch_embed.proj.weight.data\n",
    "        # trunc_normal_(w.view([w.shape[0], -1]), std=0.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, C, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] % p == 0 and imgs.shape[3] % p == 0\n",
    "\n",
    "        h = self.img_size[0] // p\n",
    "        w = self.img_size[1] // p\n",
    "        c = self.in_vars\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], c, h, p, w, p))\n",
    "        x = torch.einsum(\"nchpwq->nhwpqc\", x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * c))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size * self.upsampling\n",
    "        c = self.out_vars\n",
    "        h = self.img_out_size[0] // p\n",
    "        w = self.img_out_size[1] // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: B, C, H, W\n",
    "        \"\"\"\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)  # B, L, D\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # dropout\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    #def forward_loss(self, y, pred, out_variables, metric, lat):  # metric is a list\n",
    "    def forward_loss(self, pred):  # metric is a list\n",
    "        \"\"\"\n",
    "        y: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        \"\"\"\n",
    "        pred = self.unpatchify(pred)\n",
    "        return pred\n",
    "        #return ([m(pred, y, out_variables, lat=lat) for m in metric], pred)\n",
    "\n",
    "   # def forward(self, x, y):\n",
    "       # def forward(self, x, y, out_variables, metric, lat):\n",
    "   #     embeddings = self.forward_encoder(x)  # B, L, D\n",
    "   #     preds = self.head(embeddings)\n",
    "       # print(preds.shape)\n",
    "   #     loss = self.forward_loss(preds)\n",
    "        \n",
    "        # loss, preds = self.forward_loss(y, preds, out_variables, metric, lat)\n",
    "   #     return preds\n",
    "    def forward(self, x):\n",
    "       # def forward(self, x, y, out_variables, metric, lat):\n",
    "        embeddings = self.forward_encoder(x)  # B, L, D\n",
    "        x = self.head(embeddings)\n",
    "        x = self.unpatchify(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.forward_encoder(x)\n",
    "            pred = self.head(embeddings)\n",
    "        return self.unpatchify(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/46983267/ipykernel_15710/2479062441.py:43: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model = VisionTransformer(img_size=[32, 64], embed_dim=128, patch_size=2, in_vars = 2, out_vars = 2, depth=8, upsampling=1).to(device)\n",
    "#x, y = torch.randn(2, 2, 32, 64), torch.randn(2, 2, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = Xt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 32, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(Xt).to(device)\n",
    "#pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yt = torch.as_tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "learning_rate = 0.008\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.SGD(params = model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.0\n",
    "val_loss = 0.0\n",
    "\n",
    "trainingEpoch_loss = []\n",
    "validationEpoch_loss = []\n",
    "\n",
    "size = len(dg_train.data)\n",
    "\n",
    "  # Run the training loop\n",
    "for epoch in range(0, num_epochs): # 5 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = []\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for batch, (X,y) in enumerate(dg_train):\n",
    "        \n",
    "        Xt = torch.as_tensor(X).to(device)\n",
    "        yt = torch.as_tensor(y).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(Xt).to(device)\n",
    "        # compute loss \n",
    "        #loss = model(Xt,yt).to(device)\n",
    "        #loss = torch.as_tensor(loss.mean())\n",
    "        \n",
    "        loss = loss_fn(pred, yt).to(device)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "      \n",
    "        # Print statistics\n",
    "        current_loss.append(loss.item())\n",
    "        \n",
    "    trainingEpoch_loss.append(np.array(current_loss).mean())\n",
    "        \n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for batch, (X,y) in enumerate(dg_valid):\n",
    "            \n",
    "                validationStep_loss = []\n",
    "                \n",
    "                Xt = torch.as_tensor(X).to(device)\n",
    "                yt = torch.as_tensor(y).to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                # pred_val = model.predict(Xt).to(device)\n",
    "                # compute loss \n",
    "                #print('calculate validation loss')\n",
    "                # val_loss = model(Xt,yt).to(device)\n",
    "                #validation_loss = torch.as_tensor(val_loss.mean())\n",
    "                #validationStep_loss.append(validation_loss.item())\n",
    "                \n",
    "                pred_val = model(Xt).to(device)\n",
    "                # compute loss \n",
    "                validation_loss = loss_fn(pred_val, yt).to(device)\n",
    "                validationStep_loss.append(validation_loss.item())\n",
    "\n",
    "            \n",
    "        validationEpoch_loss.append(np.array(validationStep_loss).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f16903ac910>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(trainingEpoch_loss)+1),trainingEpoch_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(validationEpoch_loss)+1),validationEpoch_loss,label='Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTT",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "89acc57343a2b9c233d3a03eb936ce8fe371fffead8157434df0be7232f9cdb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
