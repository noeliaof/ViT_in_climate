{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c797a559-8cc6-4507-9726-9a211dfd4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/no21h426/.conda/envs/pyTT/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import Block, PatchEmbed, trunc_normal_\n",
    "\n",
    "from utils.pos_embed import (\n",
    "    get_1d_sincos_pos_embed_from_grid,\n",
    "    get_2d_sincos_pos_embed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80646a0-fe71-49be-bea1-56ac12dab73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 13:55:29.776332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 13:55:31.511223: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software.el7/software/cuDNN/8.2.1.32-CUDA-11.3.1/lib:/software.el7/software/CUDA/11.3.1/nvvm/lib64:/software.el7/software/CUDA/11.3.1/extras/CUPTI/lib64:/software.el7/software/CUDA/11.3.1/lib:/software.el7/software/Python/3.9.5-GCCcore-10.3.0/lib:/software.el7/software/OpenSSL/1.1/lib:/software.el7/software/libffi/3.3-GCCcore-10.3.0/lib64:/software.el7/software/GMP/6.2.1-GCCcore-10.3.0/lib:/software.el7/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/software.el7/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/software.el7/software/libreadline/8.1-GCCcore-10.3.0/lib:/software.el7/software/ncurses/6.2-GCCcore-10.3.0/lib:/software.el7/software/zlib/1.2.11-GCCcore-10.3.0/lib:/software.el7/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/software.el7/software/HDF5/1.12.1-gompi-2021a/lib:/software.el7/software/Szip/2.1.1-GCCcore-10.3.0/lib:/software.el7/software/ScaLAPACK/2.1.0-gompi-2021a-fb/lib:/software.el7/software/FFTW/3.3.9-gompi-2021a/lib:/software.el7/software/FlexiBLAS/3.0.4-GCC-10.3.0/lib:/software.el7/software/OpenBLAS/0.3.15-GCC-10.3.0/lib:/software.el7/software/OpenMPI/4.1.1-GCC-10.3.0/lib:/software.el7/software/libfabric/1.12.1-GCCcore-10.3.0/lib:/software.el7/software/UCX/1.10.0-GCCcore-10.3.0/lib:/software.el7/software/hwloc/2.4.1-GCCcore-10.3.0/lib:/software.el7/software/libpciaccess/0.16-GCCcore-10.3.0/lib:/software.el7/software/libxml2/2.9.10-GCCcore-10.3.0/lib:/software.el7/software/XZ/5.2.5-GCCcore-10.3.0/lib:/software.el7/software/numactl/2.0.14-GCCcore-10.3.0/lib:/software.el7/software/binutils/2.36.1-GCCcore-10.3.0/lib:/software.el7/software/GCCcore/10.3.0/lib64\n",
      "2023-02-10 13:55:31.511475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software.el7/software/cuDNN/8.2.1.32-CUDA-11.3.1/lib:/software.el7/software/CUDA/11.3.1/nvvm/lib64:/software.el7/software/CUDA/11.3.1/extras/CUPTI/lib64:/software.el7/software/CUDA/11.3.1/lib:/software.el7/software/Python/3.9.5-GCCcore-10.3.0/lib:/software.el7/software/OpenSSL/1.1/lib:/software.el7/software/libffi/3.3-GCCcore-10.3.0/lib64:/software.el7/software/GMP/6.2.1-GCCcore-10.3.0/lib:/software.el7/software/SQLite/3.35.4-GCCcore-10.3.0/lib:/software.el7/software/Tcl/8.6.11-GCCcore-10.3.0/lib:/software.el7/software/libreadline/8.1-GCCcore-10.3.0/lib:/software.el7/software/ncurses/6.2-GCCcore-10.3.0/lib:/software.el7/software/zlib/1.2.11-GCCcore-10.3.0/lib:/software.el7/software/bzip2/1.0.8-GCCcore-10.3.0/lib:/software.el7/software/HDF5/1.12.1-gompi-2021a/lib:/software.el7/software/Szip/2.1.1-GCCcore-10.3.0/lib:/software.el7/software/ScaLAPACK/2.1.0-gompi-2021a-fb/lib:/software.el7/software/FFTW/3.3.9-gompi-2021a/lib:/software.el7/software/FlexiBLAS/3.0.4-GCC-10.3.0/lib:/software.el7/software/OpenBLAS/0.3.15-GCC-10.3.0/lib:/software.el7/software/OpenMPI/4.1.1-GCC-10.3.0/lib:/software.el7/software/libfabric/1.12.1-GCCcore-10.3.0/lib:/software.el7/software/UCX/1.10.0-GCCcore-10.3.0/lib:/software.el7/software/hwloc/2.4.1-GCCcore-10.3.0/lib:/software.el7/software/libpciaccess/0.16-GCCcore-10.3.0/lib:/software.el7/software/libxml2/2.9.10-GCCcore-10.3.0/lib:/software.el7/software/XZ/5.2.5-GCCcore-10.3.0/lib:/software.el7/software/numactl/2.0.14-GCCcore-10.3.0/lib:/software.el7/software/binutils/2.36.1-GCCcore-10.3.0/lib:/software.el7/software/GCCcore/10.3.0/lib64\n",
      "2023-02-10 13:55:31.511497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "#from torchtext import data, datasets\n",
    "#from torchtext.vocab import Vocab\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Custom imports\n",
    "from utils.datagenerator import *\n",
    "from utils.util_data import * \n",
    "from utils.metrics import *\n",
    "from utils.helpers import *\n",
    "from utils.drop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004aac31-3b4e-4e9c-96fb-1186cca0fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Avaliable : True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d8e858-c575-44d3-ab6c-984fb9a01477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data from WeatherBench\n",
    "DATADIR = '/storage/homefs/no21h426/WeatherBench-master/data/WeatherBench/5.625deg/'\n",
    "# Load the entire dataset\n",
    "z500 = xr.open_mfdataset(f'{DATADIR}geopotential/*.nc', combine='by_coords').z\n",
    "t850 = xr.open_mfdataset(f'{DATADIR}temperature/*.nc', combine='by_coords').t.drop('level')\n",
    "ds = xr.merge([z500, t850])\n",
    "lat = z500.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f4a6a5-5016-4bd2-ad8f-ec66dc0ab2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice('2015', '2016'))  \n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73ae7de-fb0f-4e78-8ac4-c1d3031036d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})\n",
    "lead_time =1\n",
    "bs = 32\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2015')), dic, lead_time, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2016', '2016')), dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c73d8-079b-44b1-ac83-4dac39e76894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 32, 64)\n",
      "torch.Size([32, 2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "X,y=dg_train[0]\n",
    "print(X.shape)\n",
    "Xt = torch.as_tensor(X)\n",
    "yt = torch.as_tensor(y)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95be50a3-95af-4d52-8c61-7d8d97761603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying\n",
    "\n",
    "class ClimaX(nn.Module):\n",
    "    \"\"\"Implements the ClimaX model as described in the paper,\n",
    "    https://arxiv.org/abs/2301.10343\n",
    "    Args:\n",
    "        default_vars (list): list of default variables to be used for training\n",
    "        img_size (list): image size of the input data\n",
    "        patch_size (int): patch size of the input data\n",
    "        embed_dim (int): embedding dimension\n",
    "        depth (int): number of transformer layers\n",
    "        decoder_depth (int): number of decoder layers\n",
    "        num_heads (int): number of attention heads\n",
    "        mlp_ratio (float): ratio of mlp hidden dimension to embedding dimension\n",
    "        drop_path (float): stochastic depth rate\n",
    "        drop_rate (float): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_vars,\n",
    "        out_vars,\n",
    "        img_size=[32, 64],\n",
    "        patch_size=2,\n",
    "        lead_times =1,\n",
    "        embed_dim=1024,\n",
    "        depth=8,\n",
    "        decoder_depth=2,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: remove time_history parameter\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.out_vars = out_vars\n",
    "        self.default_vars = default_vars\n",
    "        self.lead_times = torch.as_tensor(lead_times)\n",
    "\n",
    "        # variable tokenization: separate embedding layer for each input variable\n",
    "        self.token_embeds = nn.ModuleList(\n",
    "            [PatchEmbed(img_size, patch_size, 1, embed_dim) for i in range(len(default_vars))]\n",
    "        )\n",
    "        self.num_patches = self.token_embeds[0].num_patches\n",
    "\n",
    "        # variable embedding to denote which variable each token belongs to\n",
    "        # helps in aggregating variables\n",
    "        self.var_embed, self.var_map = self.create_var_embedding(embed_dim)\n",
    "\n",
    "        # variable aggregation: a learnable query and a single-layer cross attention\n",
    "        self.var_query = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        self.var_agg = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "        # positional embedding and lead time embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim), requires_grad=True)\n",
    "        self.lead_time_embed = nn.Linear(1, embed_dim)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # ViT backbone\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    drop=drop_rate,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # prediction head\n",
    "        self.head = nn.ModuleList()\n",
    "        for _ in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(nn.Linear(embed_dim, len(self.default_vars) * patch_size**2))\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialize pos_emb and var_emb\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.img_size[0] / self.patch_size),\n",
    "            int(self.img_size[1] / self.patch_size),\n",
    "            cls_token=False,\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        var_embed = get_1d_sincos_pos_embed_from_grid(self.var_embed.shape[-1], np.arange(len(self.default_vars)))\n",
    "        self.var_embed.data.copy_(torch.from_numpy(var_embed).float().unsqueeze(0))\n",
    "\n",
    "        # token embedding layer\n",
    "        for i in range(len(self.token_embeds)):\n",
    "            w = self.token_embeds[i].proj.weight.data\n",
    "            trunc_normal_(w.view([w.shape[0], -1]), std=0.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def create_var_embedding(self, dim):\n",
    "        var_embed = nn.Parameter(torch.zeros(1, len(self.default_vars), dim), requires_grad=True)\n",
    "        # TODO: create a mapping from var --> idx\n",
    "        var_map = {}\n",
    "        idx = 0\n",
    "        for var in self.default_vars:\n",
    "            var_map[var] = idx\n",
    "            idx += 1\n",
    "        return var_embed, var_map\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_var_ids(self, vars, device):\n",
    "        ids = np.array([self.var_map[var] for var in vars])\n",
    "        return torch.from_numpy(ids).to(device)\n",
    "\n",
    "    def get_var_emb(self, var_emb, vars):\n",
    "        ids = self.get_var_ids(vars, var_emb.device)\n",
    "        return var_emb[:, ids, :]\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, L, V * patch_size**2)\n",
    "        return imgs: (B, V, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        c = len(self.default_vars)\n",
    "        h = self.img_size[0] // p\n",
    "        w = self.img_size[1] // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "\n",
    "    def aggregate_variables(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: B, V, L, D\n",
    "        \"\"\"\n",
    "        b, _, l, _ = x.shape\n",
    "        x = torch.einsum(\"bvld->blvd\", x)\n",
    "        x = x.flatten(0, 1)  # BxL, V, D\n",
    "\n",
    "        var_query = self.var_query.repeat_interleave(x.shape[0], dim=0)\n",
    "        x, _ = self.var_agg(var_query, x, x)  # BxL, D\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = x.unflatten(dim=0, sizes=(b, l))  # B, L, D\n",
    "        return x\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor, lead_times: torch.Tensor, variables):\n",
    "        # x: `[B, V, H, W]` shape.\n",
    "\n",
    "        if isinstance(variables, list):\n",
    "            variables = tuple(variables)\n",
    "\n",
    "        # tokenize each variable separately\n",
    "        embeds = []\n",
    "        var_ids = self.get_var_ids(variables, x.device)\n",
    "        for i in range(len(var_ids)):\n",
    "            id = var_ids[i]\n",
    "            embeds.append(self.token_embeds[id](x[:, i : i + 1]))\n",
    "        x = torch.stack(embeds, dim=1)  # B, V, L, D\n",
    "\n",
    "        # add variable embedding\n",
    "        var_embed = self.get_var_emb(self.var_embed, variables)\n",
    "        x = x + var_embed.unsqueeze(2)  # B, V, L, D\n",
    "\n",
    "        # variable aggregation\n",
    "        x = self.aggregate_variables(x)  # B, L, D\n",
    "\n",
    "        # add pos embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # add lead time embedding\n",
    "     \n",
    "        #lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1))  # B, D\n",
    "        lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1))\n",
    "        lead_time_emb = lead_time_emb.unsqueeze(1)\n",
    "        x = x + lead_time_emb  # B, L, D\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, y, lead_times):\n",
    "   # def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n",
    "        \"\"\"Forward pass through the model.\n",
    "        Args:\n",
    "            x: `[B, Vi, H, W]` shape. Input weather/climate variables\n",
    "            y: `[B, Vo, H, W]` shape. Target weather/climate variables\n",
    "            lead_times: `[B]` shape. Forecasting lead times of each element of the batch.\n",
    "        Returns:\n",
    "            loss (list): Different metrics.\n",
    "            preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.\n",
    "        \"\"\"\n",
    "        out_transformers = self.forward_encoder(x, lead_times, self.default_vars)  # B, L, D\n",
    "        preds = self.head(out_transformers)  # B, L, V*p*p\n",
    "\n",
    "        preds = self.unpatchify(preds)\n",
    "        out_var_ids = self.get_var_ids(tuple(self.out_variables), preds.device)\n",
    "        preds = preds[:, out_var_ids]\n",
    "\n",
    "        #if metric is None:\n",
    "        #    loss = None\n",
    "        #else:\n",
    "        #    loss = [m(preds, y, out_variables, lat) for m in metric]\n",
    "\n",
    "        return loss, preds\n",
    "\n",
    "  #  def evaluate(self, x, y, lead_times, variables, out_variables, transform, metrics, lat, clim, log_postfix):\n",
    "  #      _, preds = self.forward(x, y, lead_times, variables, out_variables, metric=None, lat=lat)\n",
    "  #      return [m(preds, y, transform, out_variables, lat, clim, log_postfix) for m in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77c5501e-4c56-419d-8a50-9dd2c57423c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model = ClimaX(['z500','t850'], out_vars = ['z500','t850'],img_size=[32, 64]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "32dd19bf-f35f-402d-9ee8-3e9f44463757",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_cuda\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43myt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlead_times\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[87], line 227\u001b[0m, in \u001b[0;36mClimaX.forward\u001b[0;34m(self, x, y, lead_times)\u001b[0m\n\u001b[1;32m    216\u001b[0m  \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, lead_times):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m     \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the model.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m     Args:\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m         x: `[B, Vi, H, W]` shape. Input weather/climate variables\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m         preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m     \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m      out_transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_vars\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B, L, D\u001b[39;00m\n\u001b[1;32m    228\u001b[0m      preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(out_transformers)  \u001b[38;5;66;03m# B, L, V*p*p\u001b[39;00m\n\u001b[1;32m    230\u001b[0m      preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpatchify(preds)\n",
      "Cell \u001b[0;32mIn[87], line 203\u001b[0m, in \u001b[0;36mClimaX.forward_encoder\u001b[0;34m(self, x, lead_times, variables)\u001b[0m\n\u001b[1;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# add lead time embedding\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1))  # B, D\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m lead_time_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlead_time_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlead_times\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m lead_time_emb \u001b[38;5;241m=\u001b[39m lead_time_emb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    205\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m lead_time_emb  \u001b[38;5;66;03m# B, L, D\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_cuda\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "pred = model(Xt.to(device),yt.to(device), torch.as_tensor(lead_times).to(device)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692da43f-e683-4eb8-9471-46f3f83109eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01f499-3461-4ad4-abeb-9a4925cdd23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTT",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
