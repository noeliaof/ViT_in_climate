{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "#from torchtext import data, datasets\n",
    "#from torchtext.vocab import Vocab\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import math\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from datagenerator import *\n",
    "from util_data import * \n",
    "from metrics import *\n",
    "from helpers import *\n",
    "from drop import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Avaliable : True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Cuda Avaliable :\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data from WeatherBench\n",
    "DATADIR = '/storage/homefs/no21h426/WeatherBench-master/data/WeatherBench/5.625deg/'\n",
    "# Load the entire dataset\n",
    "z500 = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords').z\n",
    "t850 = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').t.drop('level')\n",
    "ds = xr.merge([z500, t850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice('2015', '2016'))  \n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})\n",
    "lead_time =1\n",
    "bs = 32\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2015')), dic, lead_time, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2016', '2016')), dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2, 32, 64)\n",
      "torch.Size([32, 2, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "X,y=dg_train[0]\n",
    "print(X.shape)\n",
    "Xt = torch.as_tensor(X)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            hidden_features=None,\n",
    "            out_features=None,\n",
    "            act_layer=nn.GELU,\n",
    "            bias=True,\n",
    "            drop=0.,\n",
    "            use_conv=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n",
    "\n",
    "        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, drop=drop)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3,\n",
    "            embed_dim=768,\n",
    "            norm_layer=None,\n",
    "            flatten=True,\n",
    "            bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "       # assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
    "       # assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")\n",
    "        # need to add\n",
    "        #x = torch.as_tensor(x)\n",
    "        x = self.proj(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchE = PatchEmbed( [32,64], [16,16], 2, 64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test = torch.randn(2, 2, 32, 64)\n",
    "pt = patchE(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# 2D sine-cosine position embedding\n",
    "# References:\n",
    "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
    "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
    "# --------------------------------------------------------\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size_h, grid_size_w, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size_h, dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size_w, dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size_h, grid_size_w])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from torch.nn.init import _calculate_fan_in_and_fan_out\n",
    "# Ref.:https://github.com/rwightman/pytorch-image-models/\n",
    "\n",
    "\n",
    "def _trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    # Values are generated by using a truncated uniform distribution and\n",
    "    # then using the inverse CDF for the normal distribution.\n",
    "    # Get upper and lower cdf values\n",
    "    l = norm_cdf((a - mean) / std)\n",
    "    u = norm_cdf((b - mean) / std)\n",
    "\n",
    "    # Uniformly fill tensor with values from [l, u], then translate to\n",
    "    # [2l-1, 2u-1].\n",
    "    tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "    # Use inverse cdf transform for normal distribution to get truncated\n",
    "    # standard normal\n",
    "    tensor.erfinv_()\n",
    "\n",
    "    # Transform to proper mean, std\n",
    "    tensor.mul_(std * math.sqrt(2.))\n",
    "    tensor.add_(mean)\n",
    "\n",
    "    # Clamp to ensure it's in the proper range\n",
    "    tensor.clamp_(min=a, max=b)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are\n",
    "    applied while sampling the normal with mean/std applied, therefore a, b args\n",
    "    should be adjusted to match the range of mean, std args.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        return _trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from climate_learn\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        in_vars, \n",
    "        out_vars,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "        learn_pos_emb=False,\n",
    "        upsampling=1,\n",
    "        embed_dim=128,\n",
    "        depth=12,\n",
    "        decoder_depth=8,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        weight_init = '',\n",
    "    ):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.upsampling = upsampling\n",
    "\n",
    "        self.img_out_size = [img_size[0] * upsampling, img_size[1] * upsampling]\n",
    "        self.n_channels = in_vars\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_vars = in_vars\n",
    "        self.out_vars = out_vars\n",
    "\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT encoder\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size, patch_size, in_vars, embed_dim\n",
    "        )\n",
    "        self.num_patches = self.patch_embed.num_patches  # 128\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.num_patches, embed_dim), requires_grad=learn_pos_emb\n",
    "        )  # fixed sin-cos embedding\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path, depth)\n",
    "        ]  # stochastic depth decay rule\n",
    "           \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                 Block(\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=nn.LayerNorm,\n",
    "                    drop=drop_rate,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # Up to here, it's like I did before\n",
    "        # Next part introduces further changes \n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT prediction head\n",
    "        self.head = nn.ModuleList()\n",
    "        for i in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(\n",
    "            nn.Linear(embed_dim, self.out_vars * patch_size**2 * upsampling**2)\n",
    "        )\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "        # --------------------------------------------------------------------------\n",
    "        if weight_init != 'skip':\n",
    "            self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.pos_embed.shape[-1],\n",
    "            int(self.img_size[0] / self.patch_size),\n",
    "            int(self.img_size[1] / self.patch_size),\n",
    "            cls_token=False,\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        # w = self.patch_embed.proj.weight.data\n",
    "        # trunc_normal_(w.view([w.shape[0], -1]), std=0.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, C, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        assert imgs.shape[2] % p == 0 and imgs.shape[3] % p == 0\n",
    "\n",
    "        h = self.img_size[0] // p\n",
    "        w = self.img_size[1] // p\n",
    "        c = self.in_vars\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], c, h, p, w, p))\n",
    "        x = torch.einsum(\"nchpwq->nhwpqc\", x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * c))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size * self.upsampling\n",
    "        c = self.out_vars\n",
    "        h = self.img_out_size[0] // p\n",
    "        w = self.img_out_size[1] // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: B, C, H, W\n",
    "        \"\"\"\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)  # B, L, D\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # dropout\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    #def forward_loss(self, y, pred, out_variables, metric, lat):  # metric is a list\n",
    "    def forward_loss(self, pred):  # metric is a list\n",
    "        \"\"\"\n",
    "        y: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        \"\"\"\n",
    "        pred = self.unpatchify(pred)\n",
    "        return pred\n",
    "        #return ([m(pred, y, out_variables, lat=lat) for m in metric], pred)\n",
    "\n",
    "   # def forward(self, x, y):\n",
    "       # def forward(self, x, y, out_variables, metric, lat):\n",
    "   #     embeddings = self.forward_encoder(x)  # B, L, D\n",
    "   #     preds = self.head(embeddings)\n",
    "       # print(preds.shape)\n",
    "   #     loss = self.forward_loss(preds)\n",
    "        \n",
    "        # loss, preds = self.forward_loss(y, preds, out_variables, metric, lat)\n",
    "   #     return preds\n",
    "    def forward(self, x):\n",
    "       # def forward(self, x, y, out_variables, metric, lat):\n",
    "        embeddings = self.forward_encoder(x)  # B, L, D\n",
    "        x = self.head(embeddings)\n",
    "        x = self.unpatchify(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.forward_encoder(x)\n",
    "            pred = self.head(embeddings)\n",
    "        return self.unpatchify(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## NOT working...need to check this in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/46983267/ipykernel_8668/2479062441.py:43: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  omega = np.arange(embed_dim // 2, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model = VisionTransformer(img_size=[32, 64], embed_dim=128, patch_size=2, in_vars = 2, out_vars = 2, depth=8, upsampling=1).to(device)\n",
    "#x, y = torch.randn(2, 2, 32, 64), torch.randn(2, 2, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = Xt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 32, 64])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 11.91 GiB total capacity; 10.36 GiB already allocated; 374.31 MiB free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[56], line 186\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    185\u001b[0m    \u001b[38;5;66;03m# def forward(self, x, y, out_variables, metric, lat):\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B, L, D\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(embeddings)\n\u001b[1;32m    188\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpatchify(x)\n",
      "Cell \u001b[0;32mIn[56], line 160\u001b[0m, in \u001b[0;36mVisionTransformer.forward_encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# apply Transformer blocks\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[44], line 68\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 68\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/pyTT/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[44], line 19\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     17\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m     20\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 11.91 GiB total capacity; 10.36 GiB already allocated; 374.31 MiB free; 10.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "y = model(Xt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2, 32, 64])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(Xt).to(device)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2, 32, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = torch.as_tensor(y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 2, 32, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(Xt,yt)\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0.0\n",
    "val_loss = 0.0\n",
    "\n",
    "trainingEpoch_loss = []\n",
    "validationEpoch_loss = []\n",
    "\n",
    "size = len(dg_train.data)\n",
    "\n",
    "  # Run the training loop\n",
    "for epoch in range(0, num_epochs): # 5 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = []\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for batch, (X,y) in enumerate(dg_train):\n",
    "        \n",
    "        Xt = torch.as_tensor(X).to(device)\n",
    "        yt = torch.as_tensor(y).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model.predict(Xt).to(device)\n",
    "        # compute loss \n",
    "        loss = model(Xt,yt).to(device)\n",
    "        loss = torch.as_tensor(loss.mean())\n",
    "        #loss = loss_fn(pred, yt).to(device)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "      \n",
    "        # Print statistics\n",
    "        current_loss.append(loss.item())\n",
    "        \n",
    "    trainingEpoch_loss.append(np.array(current_loss).mean())\n",
    "        \n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        for batch, (X,y) in enumerate(dg_valid):\n",
    "            \n",
    "                validationStep_loss = []\n",
    "                \n",
    "                Xt = torch.as_tensor(X).to(device)\n",
    "                yt = torch.as_tensor(y).to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                pred_val = model.predict(Xt).to(device)\n",
    "                # compute loss \n",
    "                #print('calculate validation loss')\n",
    "                val_loss = model(Xt,yt).to(device)\n",
    "                validation_loss = torch.as_tensor(val_loss.mean())\n",
    "                validationStep_loss.append(validation_loss.item())\n",
    "            \n",
    "        validationEpoch_loss.append(np.array(validationStep_loss).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa1402d8400>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(trainingEpoch_loss)+1),trainingEpoch_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(validationEpoch_loss)+1),validationEpoch_loss,label='Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTT",
   "language": "python",
   "name": "pytt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "89acc57343a2b9c233d3a03eb936ce8fe371fffead8157434df0be7232f9cdb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
